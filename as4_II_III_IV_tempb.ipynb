{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlVgMFow8Ib0",
        "outputId": "8e935baf-6378-4b84-b4e7-7feefc132163"
      },
      "outputs": [],
      "source": [
        "from importlib import import_module\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.api._v2 import keras as KerasAPI\n",
        "keras: KerasAPI = import_module(\"tensorflow.keras\")\n",
        "print(tf.__version__)\n",
        "\n",
        "from keras import Model, layers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.utils import load_img\n",
        "from keras.utils import img_to_array\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IswFMVHz8Ib1",
        "outputId": "c3ef8a27-3bbe-4de2-edaa-ba72eae0109c"
      },
      "outputs": [],
      "source": [
        "path = ''\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/MyDrive/deepLearningAs3/'\n",
        "\n",
        "pathfinal = path + 'model_history_II_tempb/'\n",
        "pathfinal2 = path + 'model_images/'\n",
        "\n",
        "epoch_val = 3000\n",
        "batch_size_val = 32\n",
        "threshold_val = 1e-4\n",
        "inputShape = 784\n",
        "\n",
        "random_state_global = 42\n",
        "learning_rate_val = 1e-3\n",
        "\n",
        "Hidden_layer_I_N = 8\n",
        "Hidden_layer_II_N = 12\n",
        "Hidden_layer_III_N = 18\n",
        "Output_layer_N = 5\n",
        "\n",
        "Hidden_layer_ED_N = 400\n",
        "\n",
        "Hidden_layer_Activation = \"tanh\"\n",
        "Output_layer_Activation = \"softmax\"\n",
        "\n",
        "Output_layer_Encoder_Activation = \"linear\"\n",
        "\n",
        "\n",
        "epsilon_val = 1e-8\n",
        "beta_1_val = 0.9\n",
        "beta_2_val = 0.999\n",
        "\n",
        "\n",
        "class_l_r_to_d = {0:0, 1:1, 2:2, 4:3, 9:4}\n",
        "class_l_d_to_r = {0:0, 1:1, 2:2, 3:4, 4:9}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def delete_folder_contents(pathfinal):\n",
        "    folder_name = pathfinal\n",
        "    # Get all files in the folder\n",
        "    files = os.listdir(folder_name)\n",
        "\n",
        "    # Loop through the files and delete them\n",
        "    for file in files:\n",
        "        file_path = os.path.join(folder_name, file)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)\n",
        "            if os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "delete_folder_contents(pathfinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixIH2x9y8Ib1"
      },
      "source": [
        "read and saving data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnvJ1CcX8Ib2",
        "outputId": "c57204f1-b891-4888-ffd3-8266967d60ea"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "# level 0 path\n",
        "l0 = 'Group_20'\n",
        "\n",
        "DATASET = {0:pd.DataFrame(), 1:pd.DataFrame(), 2:pd.DataFrame()}\n",
        "temp_dict = {'train':0, 'val':1, 'test':2}\n",
        "\n",
        "# iterate over files in\n",
        "# that l0\n",
        "for l1 in os.listdir(l0):\n",
        "    f1 = os.path.join(l0, l1)\n",
        "    for l2 in os.listdir(f1):\n",
        "        f2 = os.path.join(f1, l2)\n",
        "        for l3 in os.listdir(f2):\n",
        "            f3 = os.path.join(f2, l3)\n",
        "            # print(f3)\n",
        "            img = load_img(f3, color_mode = \"grayscale\")\n",
        "            data_point = tf.squeeze(tf.constant(img_to_array(img)))\n",
        "            # print(\"shape:\", data_point.shape) # shape: (28, 28)\n",
        "            # data_point = data_point/255\n",
        "            # print(tf.math.reduce_min(data_point), tf.math.reduce_max(data_point))\n",
        "            # plt.imshow(data_point)\n",
        "            # plt.show()\n",
        "\n",
        "            temp = tf.reshape(data_point, shape=[-1]).numpy().tolist()\n",
        "            \n",
        "            #appending label\n",
        "            temp.append(int(l2))\n",
        "\n",
        "            # print(\"shape:\",temp.shape) #shape: (784,)\n",
        "            row = pd.Series(temp)\n",
        "            # print('1')\n",
        "            DATASET[temp_dict[l1]] = pd.concat([DATASET[temp_dict[l1]], row], axis=1)\n",
        "            # plt.imshow(tf.reshape(temp, shape=(28,28)))\n",
        "            # plt.show()\n",
        "\n",
        "\n",
        "df_train = DATASET[0].transpose()\n",
        "df_valid = DATASET[1].transpose()\n",
        "df_test = DATASET[2].transpose()\n",
        "\n",
        "## saving data\n",
        "df_train.to_csv('df_train.csv', index=False)\n",
        "df_valid.to_csv('df_valid.csv', index=False)\n",
        "df_test.to_csv('df_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4AdQBBU8Ib3"
      },
      "outputs": [],
      "source": [
        "def label_encoding(df):\n",
        "    df[df.columns[-1]] = LabelEncoder().fit_transform(df.iloc[:,-1])\n",
        "    return df\n",
        "\n",
        "def normalizing_data(df):\n",
        "    temp = df[df.columns[-1]]\n",
        "    df = df/255\n",
        "    df[df.columns[-1]] = temp\n",
        "    return df\n",
        "\n",
        "def data_visualize(df):\n",
        "  np.random.seed(random_state_global)\n",
        "  fig, axis = plt.subplots(3, 3, figsize=(6, 6))\n",
        "  axis = axis.reshape(-1)\n",
        "  for i in range(9):\n",
        "    rand_index = np.random.choice(range(len(df)))\n",
        "    axis[i].imshow(tf.reshape(df.iloc[rand_index,:-1], shape=(28,28)))\n",
        "    axis[i].set_title(f'{class_l_d_to_r[df.iloc[rand_index,-1]]}')\n",
        "    axis[i].axis(False)\n",
        "  fig.suptitle(\"Data\")\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3qcBCIO8Ib3",
        "outputId": "e7b07879-395d-4a1b-8de1-0eefe5b01b12"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(path+'df_train.csv', dtype='float32')\n",
        "df_valid = pd.read_csv(path+'df_valid.csv', dtype='float32')\n",
        "df_test = pd.read_csv(path+'df_test.csv', dtype='float32')\n",
        "\n",
        "print('df_train:', df_train.groupby(['784']).count().iloc[:,-1].to_dict())\n",
        "print('df_valid:', df_valid.groupby(['784']).count().iloc[:,-1].to_dict())\n",
        "print('df_test:', df_test.groupby(['784']).count().iloc[:,-1].to_dict())\n",
        "print()\n",
        "\n",
        "print(f'Initial Data Range: {min(df_train.iloc[:,:-1].min())} to {max(df_train.iloc[:,:-1].max())}')\n",
        "\n",
        "df_train = label_encoding(normalizing_data(df_train))\n",
        "df_valid = label_encoding(normalizing_data(df_valid))\n",
        "df_test = label_encoding(normalizing_data(df_test))\n",
        "print(f'Final Data Range: {min(df_train.iloc[:,:-1].min())} to {max(df_train.iloc[:,:-1].max())}')\n",
        "print('Label Encoded')\n",
        "\n",
        "print('Train Data',df_train.shape)\n",
        "print('Valid Data',df_valid.shape)\n",
        "print('Test Data' ,df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "I24PGZRL8Ib3",
        "outputId": "6ef2def1-f5aa-4547-cc55-87372a0a5889"
      },
      "outputs": [],
      "source": [
        "data_visualize(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtPn8eQq8Ib4"
      },
      "outputs": [],
      "source": [
        "# class StopOnThreshold(keras.callbacks.Callback):\n",
        "#     def __init__(self, threshold):\n",
        "#         super(StopOnThreshold, self).__init__()\n",
        "#         self.threshold = threshold\n",
        "#         self.previous_error = float('inf')\n",
        "    \n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         current_error = logs.get('loss')\n",
        "#         # print(f'\\nEpoch {epoch+1} curent Err:{current_error}, Previous Err:{self.previous_error}')\n",
        "#         if abs(current_error - self.previous_error) < self.threshold:\n",
        "#             self.model.stop_training = True\n",
        "#             print('\\n\\n********\\nThreshold Reached\\n********\\n')\n",
        "#         self.previous_error = current_error\n",
        "\n",
        "class ModelSaving(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.currentEpoch = 0\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.currentEpoch = epoch\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.model.save(f'{pathfinal}{self.model.name}_{self.currentEpoch+1}.tf')\n",
        "        # print(\"Training has ended!, model saved\")\n",
        "\n",
        "    \n",
        "class HistorySaver(keras.callbacks.Callback):\n",
        "    def __init__(self, initial_history):\n",
        "        super(HistorySaver, self).__init__()\n",
        "        self.history = {}\n",
        "        self.currentEpoch = 0\n",
        "        \n",
        "        for key, value in [('loss', initial_history[0]), ('accuracy', initial_history[1]), ('val_loss', initial_history[2]), ('val_accuracy', initial_history[3])]:\n",
        "            self.history.setdefault(key, []).append(value)\n",
        "        \n",
        "        # logs.items() = dict_items([('loss', 1.3612865209579468), ('accuracy', 0.46034255623817444), ('val_loss', 1.1157031059265137), ('val_accuracy', 0.6484848856925964)])\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for key, value in logs.items():\n",
        "            self.history.setdefault(key, []).append(value)\n",
        "        self.currentEpoch = epoch\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        pd.DataFrame(self.history).to_csv(f'{pathfinal}{self.model.name}_{self.currentEpoch+1}.csv', index=False)\n",
        "        # print(\"Training has ended!, model history saved\")\n",
        "\n",
        "\n",
        "class HistorySaverAE(keras.callbacks.Callback):\n",
        "    def __init__(self, initial_history):\n",
        "        super(HistorySaverAE, self).__init__()\n",
        "        self.history = {}\n",
        "        self.currentEpoch = 0\n",
        "        \n",
        "        for key, value in [('loss', initial_history[0]), ('val_loss', initial_history[1])]:\n",
        "            self.history.setdefault(key, []).append(value)\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for key, value in logs.items():\n",
        "            self.history.setdefault(key, []).append(value)\n",
        "        self.currentEpoch = epoch\n",
        "\n",
        "        \n",
        "    def on_train_end(self, logs=None):\n",
        "        pd.DataFrame(self.history).to_csv(f'{pathfinal}{self.model.name}_{self.currentEpoch+1}.csv', index=False)\n",
        "        # print(\"Training has ended!, model history saved\")\n",
        "\n",
        "\n",
        "# class EndTrainingCallback(tf.keras.callbacks.Callback):\n",
        "#     def on_train_end(self, logs=None):\n",
        "#         print(\"Training has ended!\")\n",
        "\n",
        "# create the callbacks\n",
        "\n",
        "model_saver = ModelSaving()\n",
        "\n",
        "# not initialize HistorySaver() here initialize inside function \n",
        "# stop_on_threshold = StopOnThreshold(threshold=threshold_val)\n",
        "\n",
        "# This means if for 5 epochs the accuracy has no progress on \n",
        "# the validation set then it would stop and store the previous best value.\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                                  patience=1,\n",
        "                                                  min_delta=threshold_val,\n",
        "                                                  mode='min',\n",
        "                                                  restore_best_weights=True, \n",
        "                                                  verbose=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mcBCrQKj8Ib6"
      },
      "source": [
        "# Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initializer_I = tf.keras.initializers.HeNormal(seed=random_state_global)\n",
        "# initializer_II = tf.keras.initializers.HeNormal(seed=random_state_global+1)\n",
        "# initializer_III = tf.keras.initializers.HeNormal(seed=random_state_global+2)\n",
        "# initializer_IV = tf.keras.initializers.HeNormal(seed=random_state_global+3)\n",
        "\n",
        "initializer_I = tf.keras.initializers.GlorotUniform(seed=random_state_global)\n",
        "initializer_II = tf.keras.initializers.GlorotUniform(seed=random_state_global+1)\n",
        "initializer_III = tf.keras.initializers.GlorotUniform(seed=random_state_global+2)\n",
        "initializer_IV = tf.keras.initializers.GlorotUniform(seed=random_state_global+3)\n",
        "# layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n",
        "# values = initializer(shape=(2, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Autoencoder_1h_layer(Model):\n",
        "  def __init__(self, latent_dim, m_name='model'):\n",
        "    super(Autoencoder_1h_layer, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.m_name = m_name\n",
        "\n",
        "    self.encoder = keras.Sequential([\n",
        "      layers.Dense(latent_dim, activation=Hidden_layer_Activation, kernel_initializer=initializer_I),\n",
        "    ])\n",
        "\n",
        "    self.decoder = keras.Sequential([\n",
        "      layers.Dense(inputShape, activation=Output_layer_Encoder_Activation, kernel_initializer=initializer_II),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "  # Override the name property\n",
        "  @property\n",
        "  def name(self):\n",
        "      return self.m_name\n",
        "\n",
        "class Autoencoder_3h_layer(Model):\n",
        "  def __init__(self, latent_dim, m_name='model'):\n",
        "    super(Autoencoder_3h_layer, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.m_name = m_name\n",
        "\n",
        "    self.encoder = keras.Sequential([\n",
        "      layers.Dense(Hidden_layer_ED_N, activation=Hidden_layer_Activation, kernel_initializer=initializer_I),\n",
        "      layers.Dense(latent_dim, activation=Hidden_layer_Activation, kernel_initializer=initializer_II),\n",
        "    ])\n",
        "\n",
        "    self.decoder = keras.Sequential([\n",
        "      layers.Dense(Hidden_layer_ED_N, activation=Hidden_layer_Activation, kernel_initializer=initializer_III),\n",
        "      layers.Dense(inputShape, activation=Output_layer_Encoder_Activation, kernel_initializer=initializer_IV),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "  \n",
        "  # Override the name property\n",
        "  @property\n",
        "  def name(self):\n",
        "      return self.m_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dim_list = [32, 64, 128, 256]\n",
        "autoencoder_1h_list = []\n",
        "autoencoder_3h_list = []\n",
        "for i in range(2*len(latent_dim_list)):\n",
        "    tf.random.set_seed(random_state_global)\n",
        "\n",
        "    autoencoder = None\n",
        "    if(i<len(latent_dim_list)):\n",
        "        autoencoder = Autoencoder_1h_layer(latent_dim_list[i], m_name=f'1h_layer_ae_{latent_dim_list[i%4]}')    \n",
        "    else:\n",
        "        autoencoder = Autoencoder_3h_layer(latent_dim_list[i%4], m_name=f'3h_layer_ae_{latent_dim_list[i%4]}')\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "    # Evaluate the model initial losses\n",
        "    initial_train_loss = autoencoder.evaluate(df_train.iloc[:,:-1], df_train.iloc[:,:-1], verbose=0)\n",
        "    initial_valid_loss = autoencoder.evaluate(df_valid.iloc[:,:-1], df_valid.iloc[:,:-1], verbose=0)\n",
        "\n",
        "    autoencoder.fit(df_train.iloc[:,:-1], df_train.iloc[:,:-1],\n",
        "                    epochs=epoch_val,\n",
        "                    batch_size=batch_size_val,\n",
        "                    validation_data=(df_valid.iloc[:,:-1], df_valid.iloc[:,:-1]),\n",
        "                    callbacks=[model_saver, HistorySaverAE((initial_train_loss, initial_valid_loss)), early_stopping_cb], \n",
        "                    verbose=0)\n",
        "    if(i<len(latent_dim_list)):\n",
        "        autoencoder_1h_list.append(autoencoder)\n",
        "    else:\n",
        "        autoencoder_3h_list.append(autoencoder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observe  the  average  reconstruction errors  for  the  training,  validation, and  test  data.  Average reconstruction error is computed after the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(latent_dim_list)):    \n",
        "    train_loss = autoencoder_1h_list[i].evaluate(df_train.iloc[:,:-1], df_train.iloc[:,:-1], verbose=0)\n",
        "    valid_loss = autoencoder_1h_list[i].evaluate(df_valid.iloc[:,:-1], df_valid.iloc[:,:-1], verbose=0)\n",
        "    test_loss = autoencoder_1h_list[i].evaluate(df_test.iloc[:,:-1], df_test.iloc[:,:-1], verbose=0)\n",
        "    print(f'1h_layer_{latent_dim_list[i]}: train_loss: {train_loss}, valid_loss: {valid_loss}, test_loss: {test_loss}')\n",
        "\n",
        "print()\n",
        "\n",
        "for i in range(len(latent_dim_list)):    \n",
        "    train_loss = autoencoder_3h_list[i].evaluate(df_train.iloc[:,:-1], df_train.iloc[:,:-1], verbose=0)\n",
        "    valid_loss = autoencoder_3h_list[i].evaluate(df_valid.iloc[:,:-1], df_valid.iloc[:,:-1], verbose=0)\n",
        "    test_loss = autoencoder_3h_list[i].evaluate(df_test.iloc[:,:-1], df_test.iloc[:,:-1], verbose=0)\n",
        "    print(f'3h_layer_{latent_dim_list[i]}: train_loss: {train_loss}, valid_loss: {valid_loss}, test_loss: {test_loss}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Take  one  image, from each  class,  from the training, validation,  and  test  set.  Give  their reconstructed images for each of the architectures ( along with original images). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotting_data(df, titile):\n",
        "    fig, axis = plt.subplots(9, 5, figsize=(8, 20))\n",
        "    axis = axis.reshape(-1)\n",
        "    i=0\n",
        "    axis[i+2].set_title(f'({titile}) Original Images 0, 1, 2, 4, 9')\n",
        "    for num in range(5):\n",
        "        example = tf.convert_to_tensor(df.loc[df[f'{df.shape[1]-1}'] == num].iloc[:1,:-1])\n",
        "        axis[i].imshow(tf.reshape(example, shape=(28,28)))\n",
        "        axis[i].axis(False)\n",
        "        i+=1\n",
        "\n",
        "    for m in range(4):\n",
        "        axis[i+2].set_title(f'Reconstructed Images from {latent_dim_list[m]} latent dimensions and 1 Hidden layer')\n",
        "        for num in range(5):\n",
        "            example = tf.convert_to_tensor(df.loc[df.iloc[:,-1] == num].iloc[:1,:-1])\n",
        "            encoded_img = autoencoder_1h_list[m].encoder(example)\n",
        "            decoded_img = autoencoder_1h_list[m].decoder(encoded_img).numpy()\n",
        "            axis[i].imshow(decoded_img.reshape(28, 28))\n",
        "            axis[i].axis(False)\n",
        "            i+=1\n",
        "\n",
        "    for m in range(4):\n",
        "        axis[i+2].set_title(f'Reconstructed Images from {latent_dim_list[m]} latent dimensions and 3 Hidden layer')\n",
        "        for num in range(5):\n",
        "            example = tf.convert_to_tensor(df.loc[df.iloc[:,-1] == num].iloc[:1,:-1])\n",
        "            encoded_img = autoencoder_3h_list[m].encoder(example)\n",
        "            decoded_img = autoencoder_3h_list[m].decoder(encoded_img).numpy()\n",
        "            axis[i].imshow(decoded_img.reshape(28, 28))\n",
        "            axis[i].axis(False)\n",
        "            i+=1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotting_data(df_train, 'Training Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotting_data(df_valid, 'Validation Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotting_data(df_test, 'Testing Data')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification  using  the compressed  representation  from the  1-h-l and 3-h-l  autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_X_reduced_dim_dict = {}\n",
        "df_valid_X_reduced_dim_dict = {}\n",
        "df_test_X_reduced_dim_dict = {}\n",
        "for i in range(len(latent_dim_list)):\n",
        "    df_train_X_reduced_dim_dict[f'1h_layer_{latent_dim_list[i]}'] = autoencoder_1h_list[i].encoder(tf.convert_to_tensor(df_train.iloc[:,:-1]))\n",
        "    df_train_X_reduced_dim_dict[f'3h_layer_{latent_dim_list[i]}'] = autoencoder_3h_list[i].encoder(tf.convert_to_tensor(df_train.iloc[:,:-1]))   \n",
        "    \n",
        "    df_valid_X_reduced_dim_dict[f'1h_layer_{latent_dim_list[i]}'] = autoencoder_1h_list[i].encoder(tf.convert_to_tensor(df_valid.iloc[:,:-1]))\n",
        "    df_valid_X_reduced_dim_dict[f'3h_layer_{latent_dim_list[i]}'] = autoencoder_3h_list[i].encoder(tf.convert_to_tensor(df_valid.iloc[:,:-1]))\n",
        "    \n",
        "    df_test_X_reduced_dim_dict[f'1h_layer_{latent_dim_list[i]}'] = autoencoder_1h_list[i].encoder(tf.convert_to_tensor(df_test.iloc[:,:-1]))\n",
        "    df_test_X_reduced_dim_dict[f'3h_layer_{latent_dim_list[i]}'] = autoencoder_3h_list[i].encoder(tf.convert_to_tensor(df_test.iloc[:,:-1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initializer_I = tf.keras.initializers.HeNormal(seed=random_state_global)\n",
        "# initializer_II = tf.keras.initializers.HeNormal(seed=random_state_global+1)\n",
        "# initializer_III = tf.keras.initializers.HeNormal(seed=random_state_global+2)\n",
        "# initializer_IV = tf.keras.initializers.HeNormal(seed=random_state_global+3)\n",
        "\n",
        "initializer_I = tf.keras.initializers.GlorotUniform(seed=random_state_global)\n",
        "initializer_II = tf.keras.initializers.GlorotUniform(seed=random_state_global+1)\n",
        "initializer_III = tf.keras.initializers.GlorotUniform(seed=random_state_global+2)\n",
        "initializer_IV = tf.keras.initializers.GlorotUniform(seed=random_state_global+3)\n",
        "# layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n",
        "# values = initializer(shape=(2, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FCNN_Classifier(Model):\n",
        "  def __init__(self, m_input_shape, m_name='model'):\n",
        "    super(FCNN_Classifier, self).__init__()\n",
        "    self.m_name = m_name\n",
        "    self.m_input_shape = m_input_shape\n",
        "\n",
        "    self.H_layer_1 = layers.Dense(Hidden_layer_I_N, activation=Hidden_layer_Activation, kernel_initializer=initializer_I, name=\"Hidden_layer_I\")\n",
        "    self.H_layer_2 = layers.Dense(Hidden_layer_II_N, activation=Hidden_layer_Activation, kernel_initializer=initializer_II,name=\"Hidden_layer_II\")\n",
        "    self.H_layer_3 = layers.Dense(Hidden_layer_III_N, activation=Hidden_layer_Activation, kernel_initializer=initializer_III,name=\"Hidden_layer_III\")\n",
        "    self.output_layer = layers.Dense(Output_layer_N, activation=Output_layer_Activation,kernel_initializer=initializer_IV, name=\"Output_layer\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.H_layer_1(inputs)\n",
        "    x = self.H_layer_2(x)\n",
        "    x = self.H_layer_3(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x\n",
        "  \n",
        "  def build_graph(self):\n",
        "      x = layers.Input(shape=(self.m_input_shape,), name=\"Input_layer\")\n",
        "      return Model(inputs=[x], outputs=self.call(x))\n",
        "  \n",
        "  # Override the name property\n",
        "  @property\n",
        "  def name(self):\n",
        "      return self.m_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "model_dict = {}\n",
        "history_dict = {}\n",
        "\n",
        "size_dict = len(df_train_X_reduced_dim_dict)\n",
        "for i in range(size_dict):\n",
        "  # Set random seed\n",
        "  tf.random.set_seed(random_state_global)\n",
        "  \n",
        "  input_size = latent_dim_list[int(i%(size_dict/2))]\n",
        "  \n",
        "  key = None\n",
        "  if(i<(size_dict/2)):\n",
        "    key = f'1h_layer_{input_size}'\n",
        "  else:\n",
        "    key = f'3h_layer_{input_size}'\n",
        "  \n",
        "  df_tr = df_train_X_reduced_dim_dict[key]\n",
        "  df_val = df_valid_X_reduced_dim_dict[key]\n",
        "  model = FCNN_Classifier(m_input_shape=input_size, m_name=key)\n",
        "    \n",
        "\n",
        "\n",
        "  model.build(input_shape=(None, input_size)) # assuming the input shape is (batch_size, n_components_list[i])\n",
        "  \n",
        "  # Compile the model\n",
        "  model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val, \n",
        "                                                  epsilon=epsilon_val,\n",
        "                                                  beta_1=beta_1_val,\n",
        "                                                  beta_2=beta_2_val), \n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "  # Evaluate the model initial losses\n",
        "  initial_train_loss, initial_train_acc = model.evaluate(df_tr, df_train.iloc[:,-1], verbose=0)\n",
        "  initial_valid_loss, initial_valid_acc = model.evaluate(df_val, df_valid.iloc[:,-1], verbose=0)\n",
        "\n",
        "  # Fit the model\n",
        "  history = model.fit(df_tr,\n",
        "                        df_train.iloc[:,-1],\n",
        "                        epochs=epoch_val,\n",
        "                        batch_size=batch_size_val,\n",
        "                        validation_data=(df_val, df_valid.iloc[:,-1]),\n",
        "                        callbacks=[model_saver, HistorySaver((initial_train_loss, initial_train_acc, initial_valid_loss, initial_valid_acc)), early_stopping_cb], verbose=0)\n",
        "\n",
        "  df_model_history = pd.DataFrame(history.history)\n",
        "  \n",
        "  model_dict[key] = model\n",
        "  history_dict[key] = df_model_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=10): \n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "  \n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "  \n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "             size=text_size)\n",
        "\n",
        "def makingPredictionWithCM(model, df_test_X, df_test_Y):\n",
        "    y_true = df_test_Y\n",
        "    y_prob_a = model.predict(df_test_X, verbose=0)\n",
        "    y_pred_a = y_prob_a.argmax(axis=1)\n",
        "    make_confusion_matrix(y_true, y_pred_a, classes=list(map(lambda el: class_l_d_to_r[el], [0,1,2,3,4])))\n",
        "\n",
        "\n",
        "\n",
        "def inferences(df_model_history, model, df_test_X, df_test_Y):\n",
        "    print(f'Training Accuracy for model: {df_model_history[\"accuracy\"].to_list()[-1]*100:.2f}%')\n",
        "    print(f'Validation Accuracy for model: {df_model_history[\"val_accuracy\"].to_list()[-1]*100:.2f}%')\n",
        "    print(f'Test Accuracy for model: {model.evaluate(df_test_X, df_test_Y, verbose=0)[1]*100:.2f}%')\n",
        "\n",
        "    df_model_history.plot(title=\"Accuracy / Loss vs Epoch\", xlabel='Epoch', ylabel='Accuracy / Loss')\n",
        "    plt.show()\n",
        "\n",
        "    df_model_history['loss'].plot(title=\"Average training error vs epochs\", xlabel='Epoch', ylabel='Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "size_dict = len(df_train_X_reduced_dim_dict)\n",
        "for i in range(size_dict):\n",
        "  input_size = latent_dim_list[int(i%(size_dict/2))]\n",
        "  key = None\n",
        "\n",
        "  if(i<(size_dict/2)):\n",
        "    key = f'1h_layer_{input_size}'\n",
        "  else:\n",
        "    key = f'3h_layer_{input_size}'\n",
        "\n",
        "  print('#######################################')\n",
        "  print(f'{key}_components')\n",
        "  print('#######################################')\n",
        "  makingPredictionWithCM(model_dict[key], df_test_X_reduced_dim_dict[key], df_test.iloc[:,-1])\n",
        "  inferences(history_dict[key], model_dict[key], df_test_X_reduced_dim_dict[key], df_test.iloc[:,-1])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
